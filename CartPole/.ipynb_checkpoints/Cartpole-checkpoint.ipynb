{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aaron Dharna\n",
    "#### 2/15/2019\n",
    "##### MC Prediction and Control on Cartpole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Observation`: \n",
    "\n",
    "        Type: Box(4)\n",
    "        Num\tObservation                 Min         Max\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                 -24 deg        24 deg\n",
    "        3\tPole Velocity At Tip      -Inf            Inf\n",
    "        \n",
    "`Actions`:\n",
    "\n",
    "        Type: Discrete(2)\n",
    "        Num\tAction\n",
    "        0\tPush cart to the left\n",
    "        1\tPush cart to the right\n",
    "        \n",
    "Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "    \n",
    "Reward:\n",
    "\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "        \n",
    "Starting State:\n",
    "\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "        \n",
    "Episode Termination:\n",
    "\n",
    "        Pole Angle is more than 12 degrees\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of the display)\n",
    "        Episode length is greater than 200\n",
    "        \n",
    "        Solved Requirements:\n",
    "        Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 39 timesteps\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    act=1\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        #print(action)\n",
    "        observation, reward, done, info = env.step(act)\n",
    "        #print(observation, reward, done)\n",
    "        time.sleep(0.1)\n",
    "        act = random.randint(0,1)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discritize(CVB=(-3, 3), PVB=(-3, 3)):\n",
    "    \"\"\"\n",
    "    CVB: default cart velocity bounds = -3, 3\n",
    "    PVB: default pole velocity bounds = -3, 3\n",
    "    \n",
    "    For Cartpole, we have:\n",
    "\n",
    "        Type: Box(4)\n",
    "        Num\tObservation                 Min         Max\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                 -24 deg        24 deg\n",
    "        3\tPole Velocity At Tip      -Inf            Inf\n",
    "    \"\"\"\n",
    "    assert(CVB[0] < CVB[1] and PVB[0] < PVB[1])\n",
    "    \n",
    "    cart_position_space = np.arange(-2.5, 2.7, .3)\n",
    "    cart_velocity_space = np.linspace(CVB[0], CVB[1], 5)\n",
    "    pole_angle_space = np.arange(-13, 14, 2)\n",
    "    #pole_velocity_space = np.linspace(PVB[0], PVB[1], 5)\n",
    "    \n",
    "    return cart_position_space, cart_velocity_space, pole_angle_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cps, cvs, pas = discritize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDegrees(radians):\n",
    "    return radians*(180/np.pi)\n",
    "\n",
    "def getDiscreteStateFromObs(obs):\n",
    "    poleAngleDeg = toDegrees(obs[2])\n",
    "    cartPos = None\n",
    "    poleAng = None\n",
    "    \n",
    "    #if the position of the cart is in a fail state: \n",
    "    # indicate so with a state 0 or len(cps) - 1. \n",
    "    # the other states in cps are valid and useful because certain states\n",
    "    # will have value based on how close they are to failure conditions.\n",
    "    if obs[0] <= -2.4:\n",
    "        cartPos = 0\n",
    "    elif obs[0] >= 2.4:\n",
    "        cartPos = len(cps) - 1\n",
    "    else:\n",
    "        cartPos = np.argmin(abs(obs[0] - cps))\n",
    "    \n",
    "    #same as above, but now for the pole degree. \n",
    "    if poleAngleDeg <= -12:\n",
    "        poleAng = 0\n",
    "    elif poleAngleDeg >= 12:\n",
    "        poleAng = len(pas) - 1\n",
    "    else:\n",
    "        poleAng = np.argmin(abs(poleAngleDeg - pas))\n",
    "    \n",
    "    #since there is not a terminating velocity condition, \n",
    "    # we do not need to collect failure states into a single \n",
    "    # accumulating index. \n",
    "    cartVel = np.argmin(abs(obs[1] - cvs))\n",
    "    \n",
    "    #I believe we do not need to use poleVelocity \n",
    "    # since this indicates the amount of force needed (something we do not have control over)\n",
    "    # to save the pole from continuing along it's given trajectory.\n",
    "    #poleVel = np.argmin(abs(obs[3] - pvs))\n",
    "    return (cartPos, cartVel, poleAng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01057117,  0.00711447,  0.04232744,  0.03496108])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC control\n",
    "\n",
    "Greedy in the Limit with Infinite Exploration (GLIE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_policy_creation(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Q: Our Q table. \n",
    "      Q[state] = numpy.array\n",
    "      Q[state][action] = float.\n",
    "    epsilon: small value that controls exploration.\n",
    "    nA: the number of actions available in this environment\n",
    "    \n",
    "    return: an epsilon-greedy policy specific to each state.\n",
    "    \"\"\"\n",
    "    #policy[state][action] -> probability that the action should be chosen\n",
    "    policy = defaultdict(lambda: np.ones(nA))\n",
    "    \n",
    "    #for each state in Q, build an epsilon-greedy policy\n",
    "    for state in Q.keys():\n",
    "        policy[state] *= epsilon/nA\n",
    "        bestAction = np.argmax(Q[state])\n",
    "        policy[state][bestAction] = 1 - epsilon + (epsilon/nA)\n",
    "        \n",
    "    return policy\n",
    "\n",
    "def generate_episode_from_policy(env, policy):\n",
    "    \"\"\"\n",
    "    env: the openAi gym environemnt.\n",
    "    policy: epsilon greedy policy specified for each state we have seen.\n",
    "    \n",
    "    return: a episode of the environment playing out according to our given policy.\n",
    "    \"\"\"\n",
    "    nA = env.action_space.n\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    state = getDiscreteStateFromObs(state)\n",
    "    \n",
    "    #continue until we are told to stop.\n",
    "    while True:\n",
    "        #random choice if the state has not been seen before\n",
    "        # It will get a value update on the next pass. \n",
    "        if state not in policy:\n",
    "            policy[state] *= 1/nA\n",
    "            \n",
    "        #chose an action following the policy specified for the given state we are in.\n",
    "        action = np.random.choice(np.arange(nA), p=policy[state])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        #becuase of the nature of for loops, state will persist at the start of next loop\n",
    "        # until it is overwritten by the next_state.\n",
    "        state = getDiscreteStateFromObs(next_state)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_GLIE(env, num_episodes, alpha=1.0, gamma=1.0):\n",
    "    \"\"\"\n",
    "    env: the openAi gym environemnt.\n",
    "    num_episodes: the number of episodes to train for\n",
    "    alpha: step size by which to move the Q table\n",
    "    gamma: discount factor for future rewards. \n",
    "    \n",
    "    return:\n",
    "    Optimal Q* table\n",
    "    Optimal Policy* (greedy creation from Q*)\n",
    "    \"\"\"\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionaries of arrays\n",
    "    #Q[state][action]\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    #N = defaultdict(lambda: defaultdict(int))  \n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        epsilon = max(1.0/((i_episode/8000)+1), 0.2)\n",
    "\n",
    "        policy = e_greedy_policy_creation(Q, epsilon, nA)\n",
    "        \n",
    "        episode = generate_episode_from_policy(env, policy)\n",
    "        \n",
    "        seen = []\n",
    "        \n",
    "        #If I had written every-visit MC control, I could have actually \n",
    "        # vectorized some of this code to make it faster. However, I wrote this for \n",
    "        # first visit, therefore, we need the if conditional, and cannot always count on the \n",
    "        # further enumeration. \n",
    "        \n",
    "        #for each state/action/reward triple in the episode\n",
    "        for t, (state, action, reward) in enumerate(episode):\n",
    "            \n",
    "            #if the state has not been seen before, update its value estimate\n",
    "            if state not in seen:\n",
    "                seen.append(state)\n",
    "                G = 0\n",
    "                \n",
    "                #Go to the end of the episode to get the value of the state according to current policy\n",
    "                for fi, (fstate, faction, freward) in enumerate(episode[t:]):\n",
    "                    G += (gamma**fi)*freward\n",
    "                \n",
    "                Q[state][action] += alpha*(G - Q[state][action])\n",
    "                \n",
    "    #create our policy, piPrime based on our final Q table. \n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "            \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 125000/125000.learning time:  2216.5751888751984\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "policy_glie, Q_glie = mc_control_GLIE(env, 125000, 0.01)\n",
    "print(\"learning time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_obj(policy_glie, \"policyPi_cartpole\")\n",
    "#policy_glie = load_obj(\"policyPi_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final state: (12, 2, 2)\n",
      "Episode finished after 200 timesteps\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(250):\n",
    "        env.render()\n",
    "        state = getDiscreteStateFromObs(observation)\n",
    "        action = policy_glie[state]\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        time.sleep(0.1)\n",
    "        if done:\n",
    "            print(\"final state: {}\".format(getDiscreteStateFromObs(observation)))\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
